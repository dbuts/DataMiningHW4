{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitf1254979a06c44589719673274d11d02",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4)\n",
    "<br>\n",
    "\n",
    "## 7)\n",
    ">In order to minimize the squared error when finding K clusters, the second option should appear. With the application of this option, more centroids should be in the less dense region and less centroids in the more dense regions. By having places where points are far apart, the larger number of centroids would decrease the distance from the centroids to the points and as a result, give more accurate calculations. The opposite would go for more dense regions, and as a result, giving more accurate calculations than with a large distance.\n",
    "\n",
    "## 11)\n",
    ">If the SSE of a variable is low for all the clusters, then the variable would be treated as a constant and as a result wouldn't help in seperating data into groups. If the value were high for all the clusters, the variable would more than likely just be noise compared to the rest of the data. If the value were low one cluster then it would define the cluster itself. If it were high for one cluster it most likely wouldn't be giving much information.\n",
    "<br>You can use per variable SSE information to improve your clustering by getting rid of variables that do not do a good job of differentiating the clusters. If there are low or high SSE values that exist as outliers, they can be eliminated as they either are just noise or don't give any info.\n",
    "\n",
    "## 16)\n",
    "\n",
    "## 17)\n",
    ">1) First cluster is 6, 12, 18, 24, 30 and second cluster is 42, 48. The error for the first is (12<sup>2</sup> + 6<sup>2</sup> + 0<sup>2</sup> + 6<sup>2</sup> + 12<sup>2</sup>) = 360 and for the second is (3<sup>2</sup> + 3<sup>2</sup>) = 18. Total error = 360 + 18 = 378\n",
    "<br>2) First cluster is 6, 12, 18, 24 and the second cluster is 30, 42, 48. The error for the first is (9<sup>2</sup> + 3<sup>2</sup> + 3<sup>2</sup> + 9<sup>2</sup>) = 180 and the second is (10<sup>2</sup> + 2<sup>2</sup> + 8<sup>2</sup>) = 168. Total error = 180 + 168 =348\n",
    "<br>3) Both sets of sentrods are stable solutions, so no there would be no change. \n",
    "<br>4) The clusters would be: 6, 12, 18, 24, 30 and the other 42, 48.\n",
    "<br>5) Single link seems to produce the most \"natural\" clustering.\n",
    "<br>6)\n",
    "\n",
    "## 21)\n",
    "><br>Entropy(Cluster 1): (1/693)log<sub>2</sub>(1/693) - (1/693)log<sub>2</sub>(1/693) - (11/693)log<sub>2</sub>(11/693) - (4/693)log<sub>2</sub>(4/693) - (676/693)log<sub>2</sub>(676/693) = 0.2\n",
    "<br>Entropy(Cluster 2): (27/1562)log<sub>2</sub>(27/1562) - (89/1562)log<sub>2</sub>(89/1562) - (333/1562)log<sub>2</sub>(333/1562) - (872/1562)log<sub>2</sub>(872/1562) - (253/1562)log<sub>2</sub>(253/1562) - (33/1562)log<sub>2</sub>(33/1562) = 1.84\n",
    "<br>Entropy(Cluster 3): (326/949)log<sub>2</sub>(326/949) - (465/949)log<sub>2</sub>(465/949) - (8/949)log<sub>2</sub>(8/949) - (105/949)log<sub>2</sub>(105/949) - (16/949)log<sub>2</sub>(16/949) - (29/949)log<sub>2</sub>(29/949) = 1.696\n",
    "<br>Purity(Cluster 1): 676/693 = 0.975\n",
    "<br>Purity(Cluster 2): 827/1562 = 0.529\n",
    "<br>Purity(Cluster 3): 465/949 = 0.49\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Cluster</th>\n",
    "    <th>Entropy</th>\n",
    "    <th>Purity</th>\n",
    "  </tr><tr>\n",
    "    <td>1</td>\n",
    "    <td>0.2</td>\n",
    "    <td>0.98</td>\n",
    "  </tr><tr>\n",
    "    <td>2</td>\n",
    "    <td>1.84</td>\n",
    "    <td>0.53</td>\n",
    "  </tr><tr>\n",
    "    <td>3</td>\n",
    "    <td>1.7</td>\n",
    "    <td>0.49</td>\n",
    "</table>\n",
    "\n",
    "\n",
    "## 22)\n",
    "><br>a) Yes there is a difference. The uniformly spaced points will have uniform density throughout the unit square, whereas the points generated from a uniform distribution are random so they would have places with higher or lower density.\n",
    "<br>b) The random set will typically have a lower SSE.\n",
    "<br>c) DBSCAN would do one of two things for the uniform data set: merge the points into one cluster or classify them as noise. But for the more random data, DBSCAN could potentially find clusters since there would be some variation in the density.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practicum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "#Names Attributes\n",
    "column_names = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'year', 'origin', 'name']\n",
    "all_df = pd.read_csv(\"./Data_Sets/auto-mpg.data\", delim_whitespace=True, names=column_names)\n",
    "auto_df = all_df[['mpg','displacement','horsepower','weight','acceleration']]\n",
    "\n",
    "#Finds missing HP's and replaces with numpy NaN\n",
    "missing_hp = auto_df[auto_df.horsepower == '?'].index\n",
    "auto_df.loc[missing_hp]\n",
    "auto_df.loc[missing_hp, 'horsepower'] = np.nan\n",
    "auto_df.horsepower  = auto_df.horsepower.apply(pd.to_numeric)\n",
    "\n",
    "mean_imputer =   SimpleImputer(missing_values = np.nan, strategy = 'mean')\n",
    "\n",
    "# Impute with mean and print summary\n",
    "imputed_mean_df = auto_df.copy()\n",
    "imputed_mean_df['horsepower'] = (mean_imputer.fit_transform(auto_df[['horsepower']])).ravel()\n",
    "\n",
    "\n",
    "#print(\"\\n----Mean Replaced Summary Statistics----\\n\")\n",
    "#print(imputed_mean_df.describe())\n",
    "#print(\"\\nMean Replaced Variance: \" + str(imputed_mean_df.loc[:,'horsepower'].var()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "AgglomerativeClustering(affinity='euclidean', compute_full_tree='auto',\n                        connectivity=None, distance_threshold=None,\n                        linkage='average', memory=None, n_clusters=3)"
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "clustering = AgglomerativeClustering(linkage='average',n_clusters=3,)\n",
    "X = imputed_mean_df.values\n",
    "y = imputed_mean_df.columns.values\n",
    "clusters = clustering.fit(X)\n",
    "labels = clustering.labels_\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of clusters: 3\n"
    }
   ],
   "source": [
    "print(\"Number of clusters: \" + str(clusters.n_clusters_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "cluster        mpg  displacement  horsepower       weight  acceleration\n       1  27.365414    131.934211   84.300061  2459.511278     16.298120\n       2  13.889062    358.093750  167.046875  4398.593750     13.025000\n       3  17.510294    278.985294  124.470588  3624.838235     15.105882\n"
    }
   ],
   "source": [
    "cluster_1 = X[labels==0]\n",
    "cluster_2 = X[labels==1]\n",
    "cluster_3 = X[labels==2]\n",
    "#print(\"Cluster 1 Size: \" +str(len(cluster_1)))\n",
    "#print(\"Cluster 2 Size: \" +str(len(cluster_2)))\n",
    "#print(\"Cluster 3 Size: \" +str(len(cluster_3)))\n",
    "#print(\"Sum: \" + str(len(cluster_1)+len(cluster_2)+len(cluster_3)))\n",
    "c1_means = list(cluster_1.mean(axis=0))\n",
    "c1_means.insert(0,1) # Add cluster number to front\n",
    "c2_means = list(cluster_2.mean(axis=0))\n",
    "c2_means.insert(0,2) # Add cluster number to front\n",
    "c3_means = list(cluster_3.mean(axis=0))\n",
    "c3_means.insert(0,3) # Add cluster number to front\n",
    "\n",
    "mean_df = pd.DataFrame([c1_means,c2_means,c3_means],columns =['cluster','mpg','displacement','horsepower','weight','acceleration'])\n",
    "#mean_df.concat([c2_means])\n",
    "print(mean_df.to_string(index=False))\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Variances of Clusters:\n cluster        mpg  displacement  horsepower         weight  acceleration\n       1  41.818503   2817.451499  367.755734  181945.513031      5.696801\n       2   3.306599   2104.803711  744.700928   73151.209961      3.535313\n       3   8.700041   2840.102725  702.602076   37220.282656     10.401730\n"
    }
   ],
   "source": [
    "c1_vars = list(cluster_1.var(axis=0))\n",
    "c1_vars.insert(0,1) # Add cluster number to front\n",
    "c2_vars = list(cluster_2.var(axis=0))\n",
    "c2_vars.insert(0,2) # Add cluster number to front\n",
    "c3_vars = list(cluster_3.var(axis=0))\n",
    "c3_vars.insert(0,3) # Add cluster number to front\n",
    "\n",
    "variance_df = pd.DataFrame([c1_vars,c2_vars,c3_vars],columns =['cluster','mpg','displacement','horsepower','weight','acceleration'])\n",
    "#mean_df.concat([c2_means])\n",
    "print(\"Variances of Clusters:\")\n",
    "print(variance_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now Include Origin Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "#Names Attributes\n",
    "column_names = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'year', 'origin', 'name']\n",
    "all_df = pd.read_csv(\"./Data_Sets/auto-mpg.data\", delim_whitespace=True, names=column_names)\n",
    "auto2_df = all_df[['mpg','displacement','horsepower','weight','acceleration','origin']]\n",
    "\n",
    "#Finds missing HP's and replaces with numpy NaN\n",
    "missing_hp = auto_df[auto_df.horsepower == '?'].index\n",
    "auto2_df.loc[missing_hp]\n",
    "auto2_df.loc[missing_hp, 'horsepower'] = np.nan\n",
    "auto2_df.horsepower  = auto_df.horsepower.apply(pd.to_numeric)\n",
    "\n",
    "mean_imputer =   SimpleImputer(missing_values = np.nan, strategy = 'mean')\n",
    "\n",
    "# Impute with mean and print summary\n",
    "imputed_mean_df2 = auto2_df.copy()\n",
    "imputed_mean_df2['horsepower'] = (mean_imputer.fit_transform(auto2_df[['horsepower']])).ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "AgglomerativeClustering(affinity='euclidean', compute_full_tree='auto',\n                        connectivity=None, distance_threshold=None,\n                        linkage='average', memory=None, n_clusters=3)"
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "clustering2 = AgglomerativeClustering(linkage='average',n_clusters=3,)\n",
    "X2 = imputed_mean_df2.values\n",
    "y2 = imputed_mean_df2.columns.values\n",
    "clusters2 = clustering2.fit(X2)\n",
    "labels2 = clustering2.labels_\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of clusters: 3\n"
    }
   ],
   "source": [
    "print(\"Number of clusters: \" + str(clusters2.n_clusters_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "cluster        mpg  displacement  horsepower       weight  acceleration    origin\n       1  27.365414    131.934211   84.300061  2459.511278     16.298120  1.845865\n       2  13.889062    358.093750  167.046875  4398.593750     13.025000  1.000000\n       3  17.510294    278.985294  124.470588  3624.838235     15.105882  1.044118\n"
    }
   ],
   "source": [
    "cluster_12 = X2[labels==0]\n",
    "cluster_22 = X2[labels==1]\n",
    "cluster_32 = X2[labels==2]\n",
    "#print(\"Cluster 1 Size: \" +str(len(cluster_1)))\n",
    "#print(\"Cluster 2 Size: \" +str(len(cluster_2)))\n",
    "#print(\"Cluster 3 Size: \" +str(len(cluster_3)))\n",
    "#print(\"Sum: \" + str(len(cluster_1)+len(cluster_2)+len(cluster_3)))\n",
    "c1_means2 = list(cluster_12.mean(axis=0))\n",
    "c1_means2.insert(0,1) # Add cluster number to front\n",
    "c2_means2 = list(cluster_22.mean(axis=0))\n",
    "c2_means2.insert(0,2) # Add cluster number to front\n",
    "c3_means2= list(cluster_32.mean(axis=0))\n",
    "c3_means2.insert(0,3) # Add cluster number to front\n",
    "\n",
    "mean_df2 = pd.DataFrame([c1_means2,c2_means2,c3_means2],columns =['cluster','mpg','displacement','horsepower','weight','acceleration','origin'])\n",
    "#mean_df.concat([c2_means])\n",
    "print(mean_df2.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to the clusters without origin calculated previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "cluster        mpg  displacement  horsepower       weight  acceleration\n       1  27.365414    131.934211   84.300061  2459.511278     16.298120\n       2  13.889062    358.093750  167.046875  4398.593750     13.025000\n       3  17.510294    278.985294  124.470588  3624.838235     15.105882\n"
    }
   ],
   "source": [
    "print(mean_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the clusters are identical, and cluster assignment is not affected by class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Variances of Clusters:\n cluster        mpg  displacement  horsepower         weight  acceleration    origin\n       1  41.818503   2817.451499  367.755734  181945.513031      5.696801  0.724363\n       2   3.306599   2104.803711  744.700928   73151.209961      3.535313  0.000000\n       3   8.700041   2840.102725  702.602076   37220.282656     10.401730  0.042171\n"
    }
   ],
   "source": [
    "c1_vars2 = list(cluster_12.var(axis=0))\n",
    "c1_vars2.insert(0,1) # Add cluster number to front\n",
    "c2_vars2 = list(cluster_22.var(axis=0))\n",
    "c2_vars2.insert(0,2) # Add cluster number to front\n",
    "c3_vars2 = list(cluster_32.var(axis=0))\n",
    "c3_vars2.insert(0,3) # Add cluster number to front\n",
    "\n",
    "variance_df2 = pd.DataFrame([c1_vars2,c2_vars2,c3_vars2],columns =['cluster','mpg','displacement','horsepower','weight','acceleration','origin'])\n",
    "#mean_df.concat([c2_means])\n",
    "print(\"Variances of Clusters:\")\n",
    "print(variance_df2.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's compare variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Variances of Clusters:\n cluster        mpg  displacement  horsepower         weight  acceleration\n       1  41.818503   2817.451499  367.755734  181945.513031      5.696801\n       2   3.306599   2104.803711  744.700928   73151.209961      3.535313\n       3   8.700041   2840.102725  702.602076   37220.282656     10.401730\n"
    }
   ],
   "source": [
    "c1_vars = list(cluster_1.var(axis=0))\n",
    "c1_vars.insert(0,1) # Add cluster number to front\n",
    "c2_vars = list(cluster_2.var(axis=0))\n",
    "c2_vars.insert(0,2) # Add cluster number to front\n",
    "c3_vars = list(cluster_3.var(axis=0))\n",
    "c3_vars.insert(0,3) # Add cluster number to front\n",
    "\n",
    "variance_df = pd.DataFrame([c1_vars,c2_vars,c3_vars],columns =['cluster','mpg','displacement','horsepower','weight','acceleration'])\n",
    "#mean_df.concat([c2_means])\n",
    "print(\"Variances of Clusters:\")\n",
    "print(variance_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's identical as would be expected with the means being identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "\n",
    "boston = load_boston()\n",
    "df = pd.DataFrame(data=boston.data, columns=boston.feature_names)\n",
    "df_scaled = pd.DataFrame(data=preprocessing.scale(boston.data), columns=boston.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Silhouette Score for 2 Clusters:\n0.36011768587358606\n"
    }
   ],
   "source": [
    "model2 = KMeans(n_clusters=2, init='k-means++')\n",
    "labels2 = model2.fit_predict(df_scaled)\n",
    "silhouette = metrics.silhouette_score(df_scaled, labels2)\n",
    "\n",
    "print(\"Silhouette Score for 2 Clusters:\")\n",
    "print(silhouette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Silhouette Score for 3 Clusters:\n0.2574894522739468\n"
    }
   ],
   "source": [
    "model3 = KMeans(n_clusters=3, init='k-means++')\n",
    "labels3 = model3.fit_predict(df_scaled)\n",
    "silhouette = metrics.silhouette_score(df_scaled, labels3)\n",
    "\n",
    "print(\"Silhouette Score for 3 Clusters:\")\n",
    "print(silhouette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Silhouette Score for 4 Clusters:\n0.36011768587358606\n"
    }
   ],
   "source": [
    "model4 = KMeans(n_clusters=4, init='k-means++')\n",
    "labels4 = model.fit_predict(df_scaled)\n",
    "silhouette = metrics.silhouette_score(df_scaled, labels4)\n",
    "\n",
    "print(\"Silhouette Score for 4 Clusters:\")\n",
    "print(silhouette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Silhouette Score for 5 Clusters:\n0.24783915229552517\n"
    }
   ],
   "source": [
    "model5 = KMeans(n_clusters=5, init='k-means++')\n",
    "labels5 = model5.fit_predict(df_scaled)\n",
    "silhouette = metrics.silhouette_score(df_scaled, labels5)\n",
    "\n",
    "print(\"Silhouette Score for 5 Clusters:\")\n",
    "print(silhouette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Silhouette Score for 6 Clusters:\n0.28497980790216176\n"
    }
   ],
   "source": [
    "model6 = KMeans(n_clusters=6, init='k-means++')\n",
    "labels6 = model6.fit_predict(df_scaled)\n",
    "silhouette = metrics.silhouette_score(df_scaled, labels6)\n",
    "\n",
    "print(\"Silhouette Score for 6 Clusters:\")\n",
    "print(silhouette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Average values for each cluster where n_cluster = 2\n\n Cluster      CRIM        ZN     INDUS      CHAS       NOX        RM       AGE       DIS       RAD       TAX   PTRATIO         B     LSTAT\n       1 -0.390124  0.262392 -0.620368  0.002912 -0.584675  0.243315 -0.435108  0.457222 -0.583801 -0.631460 -0.285808  0.326451 -0.446421\n       2  0.725146 -0.487722  1.153113 -0.005412  1.086769 -0.452263  0.808760 -0.849865  1.085145  1.173731  0.531248 -0.606793  0.829787\n"
    }
   ],
   "source": [
    "optimalMeans1 = list(np.mean(df_scaled[labels2 == 0], axis =0))\n",
    "optimalMeans2 = list(np.mean(df_scaled[labels2 == 1], axis =0))\n",
    "\n",
    "optimalMeans1.insert(0,1) # Add cluster number to front\n",
    "optimalMeans2.insert(0,2) # Add cluster number to front\n",
    "\n",
    "column_names = list(boston.feature_names)\n",
    "mean_df3 = pd.DataFrame([optimalMeans1, optimalMeans2], columns=['Cluster']+ column_names)\n",
    "\n",
    "print(\"Average values for each cluster where n_cluster = 2\\n\")\n",
    "print(mean_df3.to_string(index=False))\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on gathered data, the optimal number of clusters was 2, with 3 being the worst. The mean values of each feature for both clusters is shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1\n 1 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1\n 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
    }
   ],
   "source": [
    "from sklearn.metrics.cluster import homogeneity_score\n",
    "from sklearn.metrics.cluster import completeness_score\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "\n",
    "wine = load_wine()\n",
    "df = pd.DataFrame(data=wine.data, columns=wine.feature_names)\n",
    "df_scaled = pd.DataFrame(data=preprocessing.scale(wine.data), columns=wine.feature_names)\n",
    "wine_model = KMeans(n_clusters=3, init='k-means++')\n",
    "wine_labels = model.fit_predict(df)\n",
    "print(wine_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Homogeneity Score:\n0.1634165118531094\n\nCompleteness Score:\n0.1540691568070359\n"
    }
   ],
   "source": [
    "target_values = wine.target\n",
    "target_labels = pd.qcut(x=target_values,q=2,labels=[1,2])\n",
    "class_labels = target_labels.astype('int32') - 1\n",
    "\n",
    "print(\"Homogeneity Score:\")\n",
    "print(homogeneity_score(class_labels, wine_labels))\n",
    "\n",
    "print(\"\\nCompleteness Score:\")\n",
    "print(completeness_score(class_labels, wine_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}