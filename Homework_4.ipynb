{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitf1254979a06c44589719673274d11d02",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7)\n",
    "> We want to do #2 for minimal error. Using this method, more centroids will aapear in the less dense region and accordingly, fewer centroids will appear in the denser regions. Having areas where points are far apart causes more centroids adn therefore the net distance from centreoid would decrease and provide more accurate calculations.\n",
    "\n",
    "## 11)\n",
    "> Having a low SSE for all clusters would mean that the variable is treated like a constant and wouldn't be useful for dividing data into subgroups.\n",
    "<br>Having a high SSE for all clusters would result in more noise than meaningful data.\n",
    "<br>If the value is low for only one cluster, than it defines the cluster itself.\n",
    "<br>If the value is high fro only one cluster than it's not providing much information.\n",
    "<br>Per Variable SSE information can be used to improve clustering as poor performing variables can be removed if they don't differentiate clusters well. Outliers can be removed via Per Variable SSE.\n",
    "\n",
    "## 16)\n",
    ">Diagram for Part A:\n",
    "<br>![](./diagrams/16_a.png)\n",
    ">Diagram for Part B:\n",
    "<br>![](./diagrams/16_b.png)\n",
    "\n",
    "\n",
    "## 17)\n",
    ">a.1)<br>First Cluster: {6, 12, 18, 24, 30}\n",
    "<br>Error First: (12<sup>2</sup> + 6<sup>2</sup> + 0<sup>2</sup> + 6<sup>2</sup> + 12<sup>2</sup>) = 360\n",
    "<br>Second Cluster: {42, 48}.\n",
    "<br>Error Second: (3<sup>2</sup> + 3<sup>2</sup>) = 18.\n",
    "<br>Total Error: 378\n",
    "<br>a.2)<br>First Cluster: {6, 12, 18, 24}\n",
    "<br>Error First: 9<sup>2</sup> + 3<sup>2</sup> + 3<sup>2</sup> + 9<sup>2</sup>) = 180\n",
    "<br>Second Cluster: {30, 42, 48}\n",
    "<br>Error Second: (10<sup>2</sup> + 2<sup>2</sup> + 8<sup>2</sup>) = 168. Total error = 180 + 168 = 348\n",
    "<br>b) Both Centroids are stable.\n",
    "<br>c) {6, 12, 18, 24, 30} and {42, 48}.\n",
    "<br>d) Single link produces more \"natural\" clustering.\n",
    "<br>e) Single link results in contiguous clusters as it goes with denser groups.\n",
    "<br>f)K-means struggles to find clusters of different sizes because it tries to minimize squared error, resulting in larger, less natural, clusters.\n",
    "\n",
    "## 21)\n",
    "><br>Entropy of C1: 0.20\n",
    "<br>Entropy of C2: 1.84\n",
    "<br>Entropy of C3: 1.69\n",
    "<br>\n",
    "<br>Purity of C1: 676/693 = 0.98\n",
    "<br>Purity of C2: 827/1562 = 0.53\n",
    "<br>Purity of C3: 465/949 = 0.49\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Cluster #</th>\n",
    "    <th>Entropy</th>\n",
    "    <th>Purity</th>\n",
    "  </tr><tr>\n",
    "    <td>1</td>\n",
    "    <td>0.20</td>\n",
    "    <td>0.98</td>\n",
    "  </tr><tr>\n",
    "    <td>2</td>\n",
    "    <td>1.84</td>\n",
    "    <td>0.53</td>\n",
    "  </tr><tr>\n",
    "    <td>3</td>\n",
    "    <td>1.70</td>\n",
    "    <td>0.49</td>\n",
    "</table>\n",
    "\n",
    "\n",
    "## 22)\n",
    "><br>a) There is a difference because they will follow different distributions. Uniformly spaced points will have uniform density unlike points randomly generated from a uniform distribution which will have varying density.\n",
    "<br>b) The Random Set.\n",
    "<br>c) DBSCAN will either merge the points into a single cluster or identify them as noise for uniform data. for the random data, it may find multiple clusters in the variations of point density.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practicum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "#Names Attributes\n",
    "column_names = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'year', 'origin', 'name']\n",
    "all_df = pd.read_csv(\"./Data_Sets/auto-mpg.data\", delim_whitespace=True, names=column_names)\n",
    "auto_df = all_df[['mpg','displacement','horsepower','weight','acceleration']]\n",
    "\n",
    "#Finds missing HP's and replaces with numpy NaN\n",
    "missing_hp = auto_df[auto_df.horsepower == '?'].index\n",
    "auto_df.loc[missing_hp]\n",
    "auto_df.loc[missing_hp, 'horsepower'] = np.nan\n",
    "auto_df.horsepower  = auto_df.horsepower.apply(pd.to_numeric)\n",
    "\n",
    "mean_imputer =   SimpleImputer(missing_values = np.nan, strategy = 'mean')\n",
    "\n",
    "# Impute with mean and print summary\n",
    "imputed_mean_df = auto_df.copy()\n",
    "imputed_mean_df['horsepower'] = (mean_imputer.fit_transform(auto_df[['horsepower']])).ravel()\n",
    "\n",
    "\n",
    "#print(\"\\n----Mean Replaced Summary Statistics----\\n\")\n",
    "#print(imputed_mean_df.describe())\n",
    "#print(\"\\nMean Replaced Variance: \" + str(imputed_mean_df.loc[:,'horsepower'].var()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "AgglomerativeClustering(affinity='euclidean', compute_full_tree='auto',\n                        connectivity=None, distance_threshold=None,\n                        linkage='average', memory=None, n_clusters=3)"
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "clustering = AgglomerativeClustering(linkage='average',n_clusters=3,)\n",
    "X = imputed_mean_df.values\n",
    "y = imputed_mean_df.columns.values\n",
    "clusters = clustering.fit(X)\n",
    "labels = clustering.labels_\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of clusters: 3\n"
    }
   ],
   "source": [
    "print(\"Number of clusters: \" + str(clusters.n_clusters_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "cluster        mpg  displacement  horsepower       weight  acceleration\n       1  27.365414    131.934211   84.300061  2459.511278     16.298120\n       2  13.889062    358.093750  167.046875  4398.593750     13.025000\n       3  17.510294    278.985294  124.470588  3624.838235     15.105882\n"
    }
   ],
   "source": [
    "cluster_1 = X[labels==0]\n",
    "cluster_2 = X[labels==1]\n",
    "cluster_3 = X[labels==2]\n",
    "#print(\"Cluster 1 Size: \" +str(len(cluster_1)))\n",
    "#print(\"Cluster 2 Size: \" +str(len(cluster_2)))\n",
    "#print(\"Cluster 3 Size: \" +str(len(cluster_3)))\n",
    "#print(\"Sum: \" + str(len(cluster_1)+len(cluster_2)+len(cluster_3)))\n",
    "c1_means = list(cluster_1.mean(axis=0))\n",
    "c1_means.insert(0,1) # Add cluster number to front\n",
    "c2_means = list(cluster_2.mean(axis=0))\n",
    "c2_means.insert(0,2) # Add cluster number to front\n",
    "c3_means = list(cluster_3.mean(axis=0))\n",
    "c3_means.insert(0,3) # Add cluster number to front\n",
    "\n",
    "mean_df = pd.DataFrame([c1_means,c2_means,c3_means],columns =['cluster','mpg','displacement','horsepower','weight','acceleration'])\n",
    "#mean_df.concat([c2_means])\n",
    "print(\"Means of Clusters:\")\n",
    "print(mean_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Variances of Clusters:\n cluster        mpg  displacement  horsepower         weight  acceleration\n       1  41.818503   2817.451499  367.755734  181945.513031      5.696801\n       2   3.306599   2104.803711  744.700928   73151.209961      3.535313\n       3   8.700041   2840.102725  702.602076   37220.282656     10.401730\n"
    }
   ],
   "source": [
    "c1_vars = list(cluster_1.var(axis=0))\n",
    "c1_vars.insert(0,1) # Add cluster number to front\n",
    "c2_vars = list(cluster_2.var(axis=0))\n",
    "c2_vars.insert(0,2) # Add cluster number to front\n",
    "c3_vars = list(cluster_3.var(axis=0))\n",
    "c3_vars.insert(0,3) # Add cluster number to front\n",
    "\n",
    "variance_df = pd.DataFrame([c1_vars,c2_vars,c3_vars],columns =['cluster','mpg','displacement','horsepower','weight','acceleration'])\n",
    "#mean_df.concat([c2_means])\n",
    "print(\"Variances of Clusters:\")\n",
    "print(variance_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now Include Origin Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "#Names Attributes\n",
    "column_names = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'year', 'origin', 'name']\n",
    "all_df = pd.read_csv(\"./Data_Sets/auto-mpg.data\", delim_whitespace=True, names=column_names)\n",
    "auto2_df = all_df[['mpg','displacement','horsepower','weight','acceleration','origin']]\n",
    "\n",
    "#Finds missing HP's and replaces with numpy NaN\n",
    "missing_hp = auto_df[auto_df.horsepower == '?'].index\n",
    "auto2_df.loc[missing_hp]\n",
    "auto2_df.loc[missing_hp, 'horsepower'] = np.nan\n",
    "auto2_df.horsepower  = auto_df.horsepower.apply(pd.to_numeric)\n",
    "\n",
    "mean_imputer =   SimpleImputer(missing_values = np.nan, strategy = 'mean')\n",
    "\n",
    "# Impute with mean and print summary\n",
    "imputed_mean_df2 = auto2_df.copy()\n",
    "imputed_mean_df2['horsepower'] = (mean_imputer.fit_transform(auto2_df[['horsepower']])).ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "AgglomerativeClustering(affinity='euclidean', compute_full_tree='auto',\n                        connectivity=None, distance_threshold=None,\n                        linkage='average', memory=None, n_clusters=3)"
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "clustering2 = AgglomerativeClustering(linkage='average',n_clusters=3,)\n",
    "X2 = imputed_mean_df2.values\n",
    "y2 = imputed_mean_df2.columns.values\n",
    "clusters2 = clustering2.fit(X2)\n",
    "labels2 = clustering2.labels_\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of clusters: 3\n"
    }
   ],
   "source": [
    "print(\"Number of clusters: \" + str(clusters2.n_clusters_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "cluster        mpg  displacement  horsepower       weight  acceleration    origin\n       1  27.365414    131.934211   84.300061  2459.511278     16.298120  1.845865\n       2  13.889062    358.093750  167.046875  4398.593750     13.025000  1.000000\n       3  17.510294    278.985294  124.470588  3624.838235     15.105882  1.044118\n"
    }
   ],
   "source": [
    "cluster_12 = X2[labels==0]\n",
    "cluster_22 = X2[labels==1]\n",
    "cluster_32 = X2[labels==2]\n",
    "#print(\"Cluster 1 Size: \" +str(len(cluster_1)))\n",
    "#print(\"Cluster 2 Size: \" +str(len(cluster_2)))\n",
    "#print(\"Cluster 3 Size: \" +str(len(cluster_3)))\n",
    "#print(\"Sum: \" + str(len(cluster_1)+len(cluster_2)+len(cluster_3)))\n",
    "c1_means2 = list(cluster_12.mean(axis=0))\n",
    "c1_means2.insert(0,1) # Add cluster number to front\n",
    "c2_means2 = list(cluster_22.mean(axis=0))\n",
    "c2_means2.insert(0,2) # Add cluster number to front\n",
    "c3_means2= list(cluster_32.mean(axis=0))\n",
    "c3_means2.insert(0,3) # Add cluster number to front\n",
    "\n",
    "mean_df2 = pd.DataFrame([c1_means2,c2_means2,c3_means2],columns =['cluster','mpg','displacement','horsepower','weight','acceleration','origin'])\n",
    "#mean_df.concat([c2_means])\n",
    "print(\"Means of Clusters:\")\n",
    "print(mean_df2.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to the clusters without origin calculated previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "cluster        mpg  displacement  horsepower       weight  acceleration\n       1  27.365414    131.934211   84.300061  2459.511278     16.298120\n       2  13.889062    358.093750  167.046875  4398.593750     13.025000\n       3  17.510294    278.985294  124.470588  3624.838235     15.105882\n"
    }
   ],
   "source": [
    "print(mean_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the clusters are identical, and cluster assignment is not affected by class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Variances of Clusters:\n cluster        mpg  displacement  horsepower         weight  acceleration    origin\n       1  41.818503   2817.451499  367.755734  181945.513031      5.696801  0.724363\n       2   3.306599   2104.803711  744.700928   73151.209961      3.535313  0.000000\n       3   8.700041   2840.102725  702.602076   37220.282656     10.401730  0.042171\n"
    }
   ],
   "source": [
    "c1_vars2 = list(cluster_12.var(axis=0))\n",
    "c1_vars2.insert(0,1) # Add cluster number to front\n",
    "c2_vars2 = list(cluster_22.var(axis=0))\n",
    "c2_vars2.insert(0,2) # Add cluster number to front\n",
    "c3_vars2 = list(cluster_32.var(axis=0))\n",
    "c3_vars2.insert(0,3) # Add cluster number to front\n",
    "\n",
    "variance_df2 = pd.DataFrame([c1_vars2,c2_vars2,c3_vars2],columns =['cluster','mpg','displacement','horsepower','weight','acceleration','origin'])\n",
    "#mean_df.concat([c2_means])\n",
    "print(\"Variances of Clusters:\")\n",
    "print(variance_df2.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's compare variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Variances of Clusters:\n cluster        mpg  displacement  horsepower         weight  acceleration\n       1  41.818503   2817.451499  367.755734  181945.513031      5.696801\n       2   3.306599   2104.803711  744.700928   73151.209961      3.535313\n       3   8.700041   2840.102725  702.602076   37220.282656     10.401730\n"
    }
   ],
   "source": [
    "c1_vars = list(cluster_1.var(axis=0))\n",
    "c1_vars.insert(0,1) # Add cluster number to front\n",
    "c2_vars = list(cluster_2.var(axis=0))\n",
    "c2_vars.insert(0,2) # Add cluster number to front\n",
    "c3_vars = list(cluster_3.var(axis=0))\n",
    "c3_vars.insert(0,3) # Add cluster number to front\n",
    "\n",
    "variance_df = pd.DataFrame([c1_vars,c2_vars,c3_vars],columns =['cluster','mpg','displacement','horsepower','weight','acceleration'])\n",
    "#mean_df.concat([c2_means])\n",
    "print(\"Variances of Clusters:\")\n",
    "print(variance_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's identical as would be expected with the means being identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "\n",
    "boston = load_boston()\n",
    "df = pd.DataFrame(data=boston.data, columns=boston.feature_names)\n",
    "df_scaled = pd.DataFrame(data=preprocessing.scale(boston.data), columns=boston.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Silhouette Score for 2 Clusters:\n0.36011768587358606\n"
    }
   ],
   "source": [
    "model2 = KMeans(n_clusters=2, init='k-means++')\n",
    "labels2 = model2.fit_predict(df_scaled)\n",
    "silhouette = metrics.silhouette_score(df_scaled, labels2)\n",
    "\n",
    "print(\"Silhouette Score for 2 Clusters:\")\n",
    "print(silhouette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Silhouette Score for 3 Clusters:\n0.2574894522739468\n"
    }
   ],
   "source": [
    "model3 = KMeans(n_clusters=3, init='k-means++')\n",
    "labels3 = model3.fit_predict(df_scaled)\n",
    "silhouette = metrics.silhouette_score(df_scaled, labels3)\n",
    "\n",
    "print(\"Silhouette Score for 3 Clusters:\")\n",
    "print(silhouette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Silhouette Score for 4 Clusters:\n0.36011768587358606\n"
    }
   ],
   "source": [
    "model4 = KMeans(n_clusters=4, init='k-means++')\n",
    "labels4 = model.fit_predict(df_scaled)\n",
    "silhouette = metrics.silhouette_score(df_scaled, labels4)\n",
    "\n",
    "print(\"Silhouette Score for 4 Clusters:\")\n",
    "print(silhouette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Silhouette Score for 5 Clusters:\n0.24783915229552517\n"
    }
   ],
   "source": [
    "model5 = KMeans(n_clusters=5, init='k-means++')\n",
    "labels5 = model5.fit_predict(df_scaled)\n",
    "silhouette = metrics.silhouette_score(df_scaled, labels5)\n",
    "\n",
    "print(\"Silhouette Score for 5 Clusters:\")\n",
    "print(silhouette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Silhouette Score for 6 Clusters:\n0.28497980790216176\n"
    }
   ],
   "source": [
    "model6 = KMeans(n_clusters=6, init='k-means++')\n",
    "labels6 = model6.fit_predict(df_scaled)\n",
    "silhouette = metrics.silhouette_score(df_scaled, labels6)\n",
    "\n",
    "print(\"Silhouette Score for 6 Clusters:\")\n",
    "print(silhouette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Average values for each cluster where n_cluster = 2\n\n Cluster      CRIM        ZN     INDUS      CHAS       NOX        RM       AGE       DIS       RAD       TAX   PTRATIO         B     LSTAT\n       1 -0.390124  0.262392 -0.620368  0.002912 -0.584675  0.243315 -0.435108  0.457222 -0.583801 -0.631460 -0.285808  0.326451 -0.446421\n       2  0.725146 -0.487722  1.153113 -0.005412  1.086769 -0.452263  0.808760 -0.849865  1.085145  1.173731  0.531248 -0.606793  0.829787\n"
    }
   ],
   "source": [
    "optimalMeans1 = list(np.mean(df_scaled[labels2 == 0], axis =0))\n",
    "optimalMeans2 = list(np.mean(df_scaled[labels2 == 1], axis =0))\n",
    "\n",
    "optimalMeans1.insert(0,1) # Add cluster number to front\n",
    "optimalMeans2.insert(0,2) # Add cluster number to front\n",
    "\n",
    "column_names = list(boston.feature_names)\n",
    "mean_df3 = pd.DataFrame([optimalMeans1, optimalMeans2], columns=['Cluster']+ column_names)\n",
    "\n",
    "print(\"Average values for each cluster where n_cluster = 2\\n\")\n",
    "print(mean_df3.to_string(index=False))\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on gathered data, the optimal number of clusters was 2, with 3 being the worst. The mean values of each feature for both clusters is shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import homogeneity_score\n",
    "from sklearn.metrics.cluster import completeness_score\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "\n",
    "wine = load_wine()\n",
    "df = pd.DataFrame(data=wine.data, columns=wine.feature_names)\n",
    "df_scaled = pd.DataFrame(data=preprocessing.scale(wine.data), columns=wine.feature_names)\n",
    "wine_model = KMeans(n_clusters=3, init='k-means++')\n",
    "wine_labels = model.fit_predict(df)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Homogeneity Score:\n0.1634165118531094\n\nCompleteness Score:\n0.1540691568070359\n"
    }
   ],
   "source": [
    "target_values = wine.target\n",
    "target_labels = pd.qcut(x=target_values,q=2,labels=[1,2])\n",
    "class_labels = target_labels.astype('int32') - 1\n",
    "\n",
    "print(\"Homogeneity Score:\")\n",
    "print(homogeneity_score(class_labels, wine_labels))\n",
    "\n",
    "print(\"\\nCompleteness Score:\")\n",
    "print(completeness_score(class_labels, wine_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Homogeneity Score describes what percentage of clusters belonging to a single class. Because of this, the score is independent of the cluster and class labels.\n",
    "\n",
    "Completeness Score describes what percentage of data points are members of a given class are in the same clusters. This too is independent of cluster and class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}